{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e932b1-198a-4823-82a3-1543da94d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 13:38:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.133:7077\") \\\n",
    "        .appName(\"PART_AB_DanielCeoca\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98b117d-6d84-4268-828d-dfb529ab526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 13:38:28 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.json('hdfs://192.168.2.133:9000/user/ubuntu/yasp-chunks_test_output/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9b5c8b-b27e-4f47-a29b-18a993b3ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=================================================>       (28 + 4) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiant win rate: 49.38%\n",
      "Dire win rate: 46.63%\n",
      "Winning percentage gap: 2.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Calclulate the times of wins each team\n",
    "radiant_wins = df.select(col(\"radiant_win\")).where(col(\"radiant_win\")).count()\n",
    "dire_wins = df.select(col(\"radiant_win\")).where(~col(\"radiant_win\")).count()\n",
    "\n",
    "# Victory percentage\n",
    "total_games = df.count()\n",
    "radiant_win_rate = radiant_wins / total_games\n",
    "dire_win_rate = dire_wins / total_games\n",
    "\n",
    "\n",
    "win_rate_diff = abs(radiant_win_rate - dire_win_rate) * 100\n",
    "\n",
    "\n",
    "print(f\"Radiant win rate: {radiant_win_rate:.2%}\")\n",
    "print(f\"Dire win rate: {dire_win_rate:.2%}\")\n",
    "print(f\"Winning percentage gap: {win_rate_diff:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6557b6-d9d0-48e1-99c1-d32fed38a42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:==============>                                         (8 + 24) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiant 10-minute gold lead win rate: 65.16%\n",
      "Dire 10-minute gold lead win rate: 62.75%\n",
      "Radiant 10-minute XP lead win rate: 63.87%\n",
      "Dire 10-minute XP lead win rate: 61.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, size\n",
    "#confirm\n",
    "df = df.withColumn(\"has_10_minutes\", size(col(\"radiant_gold_adv\")) >= 10)\n",
    "\n",
    "# Filter\n",
    "df = df.filter(col(\"has_10_minutes\"))\n",
    "\n",
    "# Extract\n",
    "df = df.withColumn(\"radiant_gold_adv_10min\", col(\"radiant_gold_adv\")[9]) \\\n",
    "       .withColumn(\"radiant_xp_adv_10min\", col(\"radiant_xp_adv\")[9])\n",
    "\n",
    "radiant_gold_lead = df.filter(col(\"radiant_gold_adv_10min\") > 0)\n",
    "dire_gold_lead = df.filter(col(\"radiant_gold_adv_10min\") < 0)\n",
    "\n",
    "radiant_xp_lead = df.filter(col(\"radiant_xp_adv_10min\") > 0)\n",
    "dire_xp_lead = df.filter(col(\"radiant_xp_adv_10min\") < 0)\n",
    "\n",
    "# Calculate win rates\n",
    "radiant_gold_lead_win_rate = radiant_gold_lead.filter(col(\"radiant_win\") == True).count() / radiant_gold_lead.count()\n",
    "dire_gold_lead_win_rate = dire_gold_lead.filter(col(\"radiant_win\") == False).count() / dire_gold_lead.count()\n",
    "\n",
    "radiant_xp_lead_win_rate = radiant_xp_lead.filter(col(\"radiant_win\") == True).count() / radiant_xp_lead.count()\n",
    "dire_xp_lead_win_rate = dire_xp_lead.filter(col(\"radiant_win\") == False).count() / dire_xp_lead.count()\n",
    "\n",
    "print(f\"Radiant 10-minute gold lead win rate: {radiant_gold_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Dire 10-minute gold lead win rate: {dire_gold_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Radiant 10-minute XP lead win rate: {radiant_xp_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Dire 10-minute XP lead win rate: {dire_xp_lead_win_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba36c7e-3520-48dc-a502-6c1451d67cdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[FIELD_NOT_FOUND] No such struct field `assists` in `account_id`, `assists, `, `deaths`, `kills`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explode, col, when, avg\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Expand the players array and calculate the KDA for each player\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_players \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplayers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKDA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplayer.kills\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplayer.assists\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplayer.deaths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplayer.deaths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\\\n\u001b[1;32m      8\u001b[0m     select(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer.account_id\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKDA\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Average KDA per game\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df_avg_kda_per_match \u001b[38;5;241m=\u001b[39m df_players\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(avg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKDA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_KDA\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:5174\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5173\u001b[0m     )\n\u001b[0;32m-> 5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [FIELD_NOT_FOUND] No such struct field `assists` in `account_id`, `assists, `, `deaths`, `kills`."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col, when, avg\n",
    "\n",
    "# Expand the players array and calculate the KDA for each player\n",
    "df_players = df.withColumn(\"player\", explode(col(\"players\"))).\\\n",
    "    withColumn(\"KDA\", \n",
    "               (col(\"player.kills\") + col(\"player.assists\")) / \n",
    "               when(col(\"player.deaths\") == 0, 1).otherwise(col(\"player.deaths\"))).\\\n",
    "    select(col(\"match_id\"), col(\"player.account_id\"), col(\"KDA\"))\n",
    "\n",
    "# Average KDA per game\n",
    "df_avg_kda_per_match = df_players.groupBy(\"match_id\").agg(avg(\"KDA\").alias(\"avg_KDA\"))\n",
    "\n",
    "\n",
    "highest_avg_kda = df_avg_kda_per_match.orderBy(col(\"avg_KDA\").desc()).first()\n",
    "lowest_avg_kda = df_avg_kda_per_match.orderBy(col(\"avg_KDA\").asc()).first()\n",
    "\n",
    "\n",
    "print(f\"The Match id with the highest KDA {highest_avg_kda['match_id']}, Average KDA: {highest_avg_kda['avg_KDA']}\")\n",
    "print(f\"The Match id with the lowest KDA: {lowest_avg_kda['match_id']}, Average KDA: {lowest_avg_kda['avg_KDA']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372f090-a4e8-4e5b-8583-596d3acb2dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:=================================================>      (28 + 4) / 32]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, sum as spark_sum\n",
    "\n",
    "df_exploded = df.withColumn(\"players\", explode(\"players\"))\n",
    "\n",
    "df_kills = df_exploded.groupBy(\"match_id\").agg(spark_sum(\"players.kills\").alias(\"total_kills\"))\n",
    "\n",
    "# Find the match with the highest total kills\n",
    "highest_kills_match = df_kills.orderBy(\"total_kills\", ascending=False).first()\n",
    "\n",
    "highest_kills_match_id = highest_kills_match[\"match_id\"]\n",
    "highest_kills = highest_kills_match[\"total_kills\"]\n",
    "\n",
    "print(f\"Match with the highest total kills: {highest_kills_match_id}, Total kills: {highest_kills}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e205eda-41af-4307-b4a2-4d20d93902c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 13:47:09 WARN HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 138510 ms exceeds timeout 120000 ms\n",
      "24/03/15 13:47:09 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 140444 ms exceeds timeout 120000 ms\n",
      "24/03/15 13:47:09 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.2.165: Executor heartbeat timed out after 138510 ms\n",
      "24/03/15 13:47:09 WARN TaskSetManager: Lost task 22.0 in stage 39.0 (TID 1510) (192.168.2.165 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 138510 ms\n",
      "24/03/15 13:47:09 WARN TaskSetManager: Lost task 30.0 in stage 39.0 (TID 1518) (192.168.2.165 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 138510 ms\n",
      "24/03/15 13:47:09 WARN TaskSetManager: Lost task 6.0 in stage 39.0 (TID 1494) (192.168.2.165 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 138510 ms\n",
      "24/03/15 13:47:09 WARN TaskSetManager: Lost task 14.0 in stage 39.0 (TID 1502) (192.168.2.165 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 138510 ms\n",
      "24/03/15 13:47:09 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.2.165: Executor heartbeat timed out after 140444 ms\n",
      "24/03/15 13:47:09 WARN TaskSetManager: Lost task 24.0 in stage 39.0 (TID 1512) (192.168.2.165 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 140444 ms\n",
      "24/03/15 13:47:09 WARN TaskSetManager: Lost task 0.0 in stage 39.0 (TID 1488) (192.168.2.165 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 140444 ms\n",
      "24/03/15 13:47:09 WARN TaskSetManager: Lost task 8.0 in stage 39.0 (TID 1496) (192.168.2.165 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 140444 ms\n",
      "24/03/15 13:47:10 WARN TaskSetManager: Lost task 16.0 in stage 39.0 (TID 1504) (192.168.2.165 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 140444 ms\n",
      "24/03/15 13:48:23 ERROR TaskSchedulerImpl: Lost executor 6 on 192.168.2.165: worker lost: Not receiving heartbeat for 60 seconds\n",
      "24/03/15 13:48:23 ERROR TaskSchedulerImpl: Lost executor 5 on 192.168.2.165: worker lost: Not receiving heartbeat for 60 seconds\n",
      "24/03/15 13:51:52 ERROR TaskSchedulerImpl: Lost executor 7 on 192.168.2.39: Command exited with code 137\n",
      "24/03/15 13:51:52 WARN TaskSetManager: Lost task 1.1 in stage 39.0 (TID 1528) (192.168.2.39 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "24/03/15 13:51:52 WARN TaskSetManager: Lost task 22.1 in stage 39.0 (TID 1527) (192.168.2.39 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "24/03/15 13:51:52 WARN TaskSetManager: Lost task 19.1 in stage 39.0 (TID 1532) (192.168.2.39 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "24/03/15 13:51:52 WARN TaskSetManager: Lost task 24.1 in stage 39.0 (TID 1523) (192.168.2.39 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Command exited with code 137\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  match_id|duration|\n",
      "+----------+--------+\n",
      "|1999816459|    7109|\n",
      "|1999966671|    6366|\n",
      "|2000865722|    6358|\n",
      "|2000816501|    5924|\n",
      "|2001015774|    5754|\n",
      "+----------+--------+\n",
      "\n",
      "Five Shortest Matches:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:====================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  match_id|duration|\n",
      "+----------+--------+\n",
      "|2001181449|     277|\n",
      "|1999937575|     324|\n",
      "|2000823661|     330|\n",
      "|2000434863|     332|\n",
      "|1999268345|     335|\n",
      "+----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "longest_matches = df.orderBy(col(\"duration\").desc()).select(\"match_id\", \"duration\").limit(5)\n",
    "\n",
    "shortest_matches = df.where(col(\"duration\") > 0).orderBy(\"duration\").select(\"match_id\", \"duration\").limit(5)\n",
    "\n",
    "print(\"Five Longest Matches:\")\n",
    "longest_matches.show()\n",
    "\n",
    "print(\"Five Shortest Matches:\")\n",
    "shortest_matches.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b865ce-2698-4537-b761-abf0aa839531",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70893ed1-4e3c-48eb-91fb-1c1ed9f541e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
