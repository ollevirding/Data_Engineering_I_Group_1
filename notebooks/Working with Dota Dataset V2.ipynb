{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e932b1-198a-4823-82a3-1543da94d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 18:22:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.133:7077\") \\\n",
    "        .appName(\"simpleAnalysisComputationTime\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98b117d-6d84-4268-828d-dfb529ab526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.json('hdfs://192.168.2.133:9000/user/ubuntu/yasp_chunk_10000_output/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9b5c8b-b27e-4f47-a29b-18a993b3ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiant win rate: 51.88%\n",
      "Dire win rate: 48.06%\n",
      "Winning percentage gap: 3.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Calclulate the times of wins each team\n",
    "radiant_wins = df.select(col(\"radiant_win\")).where(col(\"radiant_win\")).count()\n",
    "dire_wins = df.select(col(\"radiant_win\")).where(~col(\"radiant_win\")).count()\n",
    "\n",
    "# Victory percentage\n",
    "total_games = df.count()\n",
    "radiant_win_rate = radiant_wins / total_games\n",
    "dire_win_rate = dire_wins / total_games\n",
    "\n",
    "\n",
    "win_rate_diff = abs(radiant_win_rate - dire_win_rate) * 100\n",
    "\n",
    "\n",
    "print(f\"Radiant win rate: {radiant_win_rate:.2%}\")\n",
    "print(f\"Dire win rate: {dire_win_rate:.2%}\")\n",
    "print(f\"Winning percentage gap: {win_rate_diff:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6557b6-d9d0-48e1-99c1-d32fed38a42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiant 10-minute gold lead win rate: 66.37%\n",
      "Dire 10-minute gold lead win rate: 63.10%\n",
      "Radiant 10-minute XP lead win rate: 65.41%\n",
      "Dire 10-minute XP lead win rate: 61.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, size\n",
    "#confirm\n",
    "df = df.withColumn(\"has_10_minutes\", size(col(\"radiant_gold_adv\")) >= 10)\n",
    "\n",
    "# Filter\n",
    "df = df.filter(col(\"has_10_minutes\"))\n",
    "\n",
    "# Extract\n",
    "df = df.withColumn(\"radiant_gold_adv_10min\", col(\"radiant_gold_adv\")[9]) \\\n",
    "       .withColumn(\"radiant_xp_adv_10min\", col(\"radiant_xp_adv\")[9])\n",
    "\n",
    "radiant_gold_lead = df.filter(col(\"radiant_gold_adv_10min\") > 0)\n",
    "dire_gold_lead = df.filter(col(\"radiant_gold_adv_10min\") < 0)\n",
    "\n",
    "radiant_xp_lead = df.filter(col(\"radiant_xp_adv_10min\") > 0)\n",
    "dire_xp_lead = df.filter(col(\"radiant_xp_adv_10min\") < 0)\n",
    "\n",
    "# Calculate win rates\n",
    "radiant_gold_lead_win_rate = radiant_gold_lead.filter(col(\"radiant_win\") == True).count() / radiant_gold_lead.count()\n",
    "dire_gold_lead_win_rate = dire_gold_lead.filter(col(\"radiant_win\") == False).count() / dire_gold_lead.count()\n",
    "\n",
    "radiant_xp_lead_win_rate = radiant_xp_lead.filter(col(\"radiant_win\") == True).count() / radiant_xp_lead.count()\n",
    "dire_xp_lead_win_rate = dire_xp_lead.filter(col(\"radiant_win\") == False).count() / dire_xp_lead.count()\n",
    "\n",
    "print(f\"Radiant 10-minute gold lead win rate: {radiant_gold_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Dire 10-minute gold lead win rate: {dire_gold_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Radiant 10-minute XP lead win rate: {radiant_xp_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Dire 10-minute XP lead win rate: {dire_xp_lead_win_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9372f090-a4e8-4e5b-8583-596d3acb2dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match with the highest total kills: 1998243598, Total kills: 268\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, sum as spark_sum\n",
    "\n",
    "df_exploded = df.withColumn(\"players\", explode(\"players\"))\n",
    "\n",
    "df_kills = df_exploded.groupBy(\"match_id\").agg(spark_sum(\"players.kills\").alias(\"total_kills\"))\n",
    "\n",
    "# Find the match with the highest total kills\n",
    "highest_kills_match = df_kills.orderBy(\"total_kills\", ascending=False).first()\n",
    "\n",
    "highest_kills_match_id = highest_kills_match[\"match_id\"]\n",
    "highest_kills = highest_kills_match[\"total_kills\"]\n",
    "\n",
    "print(f\"Match with the highest total kills: {highest_kills_match_id}, Total kills: {highest_kills}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e205eda-41af-4307-b4a2-4d20d93902c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five Longest Matches:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  match_id|duration|\n",
      "+----------+--------+\n",
      "|1998243598|    9318|\n",
      "|1984881220|    8806|\n",
      "|1982514589|    7479|\n",
      "|1988285140|    7242|\n",
      "|1995768838|    7052|\n",
      "+----------+--------+\n",
      "\n",
      "Five Shortest Matches:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  match_id|duration|\n",
      "+----------+--------+\n",
      "|1980174434|     304|\n",
      "|1971139458|     304|\n",
      "|1965719668|     306|\n",
      "|1959749569|     308|\n",
      "|1984245435|     309|\n",
      "+----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "longest_matches = df.orderBy(col(\"duration\").desc()).select(\"match_id\", \"duration\").limit(5)\n",
    "\n",
    "shortest_matches = df.where(col(\"duration\") > 0).orderBy(\"duration\").select(\"match_id\", \"duration\").limit(5)\n",
    "\n",
    "print(\"Five Longest Matches:\")\n",
    "longest_matches.show()\n",
    "\n",
    "print(\"Five Shortest Matches:\")\n",
    "shortest_matches.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b865ce-2698-4537-b761-abf0aa839531",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70893ed1-4e3c-48eb-91fb-1c1ed9f541e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
