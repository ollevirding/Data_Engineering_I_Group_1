{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e932b1-198a-4823-82a3-1543da94d6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/13 19:18:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.133:7077\") \\\n",
    "        .appName(\"PART_AB_DanielCeoca\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7391b0b0-acf1-402c-b690-0bc9ee2db4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = spark_context.textFile('hdfs://192.168.2.133:9000/yasp-chunk-small-aa.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98b117d-6d84-4268-828d-dfb529ab526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/13 19:18:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.json('hdfs://192.168.2.133:9000/yasp-chunk-small-aa.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f9b5c8b-b27e-4f47-a29b-18a993b3ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiant win rate: 29.70%\n",
      "Dire win rate: 19.80%\n",
      "Winning percentage gap: 9.90%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Calclulate the times of wins each team\n",
    "radiant_wins = df.select(col(\"radiant_win\")).where(col(\"radiant_win\")).count()\n",
    "dire_wins = df.select(col(\"radiant_win\")).where(~col(\"radiant_win\")).count()\n",
    "\n",
    "# Victory percentage\n",
    "total_games = df.count()\n",
    "radiant_win_rate = radiant_wins / total_games\n",
    "dire_win_rate = dire_wins / total_games\n",
    "\n",
    "\n",
    "win_rate_diff = abs(radiant_win_rate - dire_win_rate) * 100\n",
    "\n",
    "\n",
    "print(f\"Radiant win rate: {radiant_win_rate:.2%}\")\n",
    "print(f\"Dire win rate: {dire_win_rate:.2%}\")\n",
    "print(f\"Winning percentage gap: {win_rate_diff:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae6557b6-d9d0-48e1-99c1-d32fed38a42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Radiant 10-minute gold lead win rate: 70.37%\n",
      "Dire 10-minute gold lead win rate: 54.55%\n",
      "Radiant 10-minute XP lead win rate: 72.41%\n",
      "Dire 10-minute XP lead win rate: 60.00%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, size\n",
    "#confirm\n",
    "df = df.withColumn(\"has_10_minutes\", size(col(\"radiant_gold_adv\")) >= 10)\n",
    "\n",
    "# Filter\n",
    "df = df.filter(col(\"has_10_minutes\"))\n",
    "\n",
    "# Extract\n",
    "df = df.withColumn(\"radiant_gold_adv_10min\", col(\"radiant_gold_adv\")[9]) \\\n",
    "       .withColumn(\"radiant_xp_adv_10min\", col(\"radiant_xp_adv\")[9])\n",
    "\n",
    "radiant_gold_lead = df.filter(col(\"radiant_gold_adv_10min\") > 0)\n",
    "dire_gold_lead = df.filter(col(\"radiant_gold_adv_10min\") < 0)\n",
    "\n",
    "radiant_xp_lead = df.filter(col(\"radiant_xp_adv_10min\") > 0)\n",
    "dire_xp_lead = df.filter(col(\"radiant_xp_adv_10min\") < 0)\n",
    "\n",
    "# Calculate win rates\n",
    "radiant_gold_lead_win_rate = radiant_gold_lead.filter(col(\"radiant_win\") == True).count() / radiant_gold_lead.count()\n",
    "dire_gold_lead_win_rate = dire_gold_lead.filter(col(\"radiant_win\") == False).count() / dire_gold_lead.count()\n",
    "\n",
    "radiant_xp_lead_win_rate = radiant_xp_lead.filter(col(\"radiant_win\") == True).count() / radiant_xp_lead.count()\n",
    "dire_xp_lead_win_rate = dire_xp_lead.filter(col(\"radiant_win\") == False).count() / dire_xp_lead.count()\n",
    "\n",
    "print(f\"Radiant 10-minute gold lead win rate: {radiant_gold_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Dire 10-minute gold lead win rate: {dire_gold_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Radiant 10-minute XP lead win rate: {radiant_xp_lead_win_rate * 100:.2f}%\")\n",
    "print(f\"Dire 10-minute XP lead win rate: {dire_xp_lead_win_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba36c7e-3520-48dc-a502-6c1451d67cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Match id with the highest KDA 2001390318, Average KDA: 16.275263157894738\n",
      "The Match id with the lowest KDA: 2001375094, Average KDA: 2.539231601731602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col, when, avg\n",
    "\n",
    "# Expand the players array and calculate the KDA for each player\n",
    "df_players = df.withColumn(\"player\", explode(col(\"players\"))).\\\n",
    "    withColumn(\"KDA\", \n",
    "               (col(\"player.kills\") + col(\"player.assists\")) / \n",
    "               when(col(\"player.deaths\") == 0, 1).otherwise(col(\"player.deaths\"))).\\\n",
    "    select(col(\"match_id\"), col(\"player.account_id\"), col(\"KDA\"))\n",
    "\n",
    "# Average KDA per game\n",
    "df_avg_kda_per_match = df_players.groupBy(\"match_id\").agg(avg(\"KDA\").alias(\"avg_KDA\"))\n",
    "\n",
    "\n",
    "highest_avg_kda = df_avg_kda_per_match.orderBy(col(\"avg_KDA\").desc()).first()\n",
    "lowest_avg_kda = df_avg_kda_per_match.orderBy(col(\"avg_KDA\").asc()).first()\n",
    "\n",
    "\n",
    "print(f\"The Match id with the highest KDA {highest_avg_kda['match_id']}, Average KDA: {highest_avg_kda['avg_KDA']}\")\n",
    "print(f\"The Match id with the lowest KDA: {lowest_avg_kda['match_id']}, Average KDA: {lowest_avg_kda['avg_KDA']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9372f090-a4e8-4e5b-8583-596d3acb2dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match with the highest total kills: 2001377504, Total kills: 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, sum as spark_sum\n",
    "\n",
    "df_exploded = df.withColumn(\"players\", explode(\"players\"))\n",
    "\n",
    "df_kills = df_exploded.groupBy(\"match_id\").agg(spark_sum(\"players.kills\").alias(\"total_kills\"))\n",
    "\n",
    "# Find the match with the highest total kills\n",
    "highest_kills_match = df_kills.orderBy(\"total_kills\", ascending=False).first()\n",
    "\n",
    "highest_kills_match_id = highest_kills_match[\"match_id\"]\n",
    "highest_kills = highest_kills_match[\"total_kills\"]\n",
    "\n",
    "print(f\"Match with the highest total kills: {highest_kills_match_id}, Total kills: {highest_kills}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e205eda-41af-4307-b4a2-4d20d93902c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five Longest Matches:\n",
      "+----------+--------+\n",
      "|  match_id|duration|\n",
      "+----------+--------+\n",
      "|2001375031|    2495|\n",
      "|2001374907|    2423|\n",
      "|2001375993|    2385|\n",
      "|2001379127|    2325|\n",
      "|2001377295|    2324|\n",
      "+----------+--------+\n",
      "\n",
      "Five Shortest Matches:\n",
      "+----------+--------+\n",
      "|  match_id|duration|\n",
      "+----------+--------+\n",
      "|2001389000|     723|\n",
      "|2001400694|     898|\n",
      "|2001387742|    1063|\n",
      "|2001389600|    1093|\n",
      "|2001378954|    1169|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "longest_matches = df.orderBy(col(\"duration\").desc()).select(\"match_id\", \"duration\").limit(5)\n",
    "\n",
    "shortest_matches = df.where(col(\"duration\") > 0).orderBy(\"duration\").select(\"match_id\", \"duration\").limit(5)\n",
    "\n",
    "print(\"Five Longest Matches:\")\n",
    "longest_matches.show()\n",
    "\n",
    "print(\"Five Shortest Matches:\")\n",
    "shortest_matches.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71b865ce-2698-4537-b761-abf0aa839531",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70893ed1-4e3c-48eb-91fb-1c1ed9f541e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
